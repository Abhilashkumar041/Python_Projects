{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbf1570-5337-4ce9-a2d7-1c5a5d2a9395",
   "metadata": {},
   "source": [
    "# Building a Character-Level Language Model for Hangman Game\n",
    "## Introduction\n",
    "\n",
    "### Steps Overview\n",
    "### Provide an overview of the key steps involved in building the language model.\n",
    "\n",
    "## Dataset\n",
    "### Describe the dataset used for training the model. Mention the source of the Hangman game dataset and the number of words available (e.g., 25,000 words).\n",
    "\n",
    "## Data Preprocessing\n",
    "### Explain the preprocessing steps performed on the dataset before training the model:\n",
    "\n",
    "### Generating input sequences: Describe how input sequences were created from each word, where each sequence contains characters from the beginning of the word up to the previous-to-last ### character.\n",
    "### Generating target characters: Explain how the target characters were derived from the last character of each input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9204353-7564-4b53-94ac-c5a6075ac1bc",
   "metadata": {},
   "source": [
    "### Character-to-Index Mapping\n",
    "#### Details the creation of a character-to-index mapping:\n",
    "\n",
    "## Data Splitting\n",
    "\n",
    "#### Describing the train-test split with a specified ratio (e.g., 80% training, 20% testing).\n",
    "#### Model Architecture\n",
    "#### Provide an overview of the model architecture chosen for the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81fd815c-1c7c-4e65-8b56-ba8909dd17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the required libraries \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137e8b7-d5c0-4a6f-a246-f9e6dec36f77",
   "metadata": {},
   "source": [
    "### Importing the file that contains the words to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f0243b-2b37-4d17-9fcd-26a780081d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_file_path = \"words_250000_train.txt\" \n",
    "with open(word_file_path, 'r') as file:\n",
    "    words = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f86aa7-8c4e-4466-b11d-d16b1a26d8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7eb75d-0b6b-4e61-872a-66fb2893d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "secret_word = random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b072e944-2475-4634-851f-2c55180a5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_letters = []\n",
    "\n",
    "max_attempts = 6\n",
    "attempts = 0\n",
    "\n",
    "current_state = \"_\" * len(secret_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1861a3d6-40de-49e8-b909-dcffebaac92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7de67da-0849-4fa3-b5cf-9a55704f0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d90625-2f4f-4537-9f51-8a77b28e2401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages (from requests) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhil\\anaconda3\\anaconda4\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528ee7ae-b63c-45fa-8d8d-b3aa84a7044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hangman_game import HangmanAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3f7f3d-afa3-48b5-8481-899b7d82ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import collections\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import parse_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402ddca-edc1-438e-b430-06b52c83605e",
   "metadata": {},
   "source": [
    "### Creating the HangmanAPI error Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "292ae9a8-426b-48ba-aba4-1d1ea147baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "\n",
    "        Exception.__init__(self, self.message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40472560-44ab-41c7-a08e-0fb55136126d",
   "metadata": {},
   "source": [
    "### Creating a Hangman API class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "615c7732-e36b-4d94-a333-633827e0050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        \n",
    "        self.current_dictionary = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
    "\n",
    "        data = {link: 0 for link in links}\n",
    "\n",
    "        for link in links:\n",
    "            requests.get(link)\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "    \n",
    "    def guess(self, word):\n",
    "        clean_word = word[::2].replace(\"_\", \".\")\n",
    "        len_word = len(clean_word)\n",
    "\n",
    "        new_dictionary = [dict_word for dict_word in self.current_dictionary if len(dict_word) == len_word and re.match(clean_word, dict_word)]\n",
    "        self.current_dictionary = new_dictionary\n",
    "\n",
    "        full_dict_string = \"\".join(new_dictionary)\n",
    "        c = collections.Counter(full_dict_string)\n",
    "        sorted_letter_count = c.most_common()\n",
    "\n",
    "        guess_letter = '!'\n",
    "        for letter, instance_count in sorted_letter_count:\n",
    "            if letter not in self.guessed_letters:\n",
    "                guess_letter = letter\n",
    "                break\n",
    "\n",
    "        if guess_letter == '!':\n",
    "            sorted_letter_count = self.full_dictionary_common_letter_sorted\n",
    "            for letter, instance_count in sorted_letter_count:\n",
    "                if letter not in self.guessed_letters:\n",
    "                    guess_letter = letter\n",
    "                    break\n",
    "\n",
    "        return guess_letter\n",
    "\n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        with open(dictionary_file_location, \"r\") as text_file:\n",
    "            full_dictionary = text_file.read().splitlines()\n",
    "        return full_dictionary\n",
    "\n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "\n",
    "        response = self.request(\"/new_game\", {\"practice\": practice})\n",
    "        if response.get('status') == \"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains > 0:\n",
    "                guess_letter = self.guess(word)\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "                try:\n",
    "                    res = self.request(\"/guess_letter\", {\"request\": \"guess_letter\", \"game_id\": game_id, \"letter\": guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "                if verbose:\n",
    "                    print(\"Server response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status == \"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status == \"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status == \"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status == \"success\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8339b-ed50-4420-b0b1-bed181223e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ece78e4-4c97-4fce-ae1b-b76b1039d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838dc2bb-c23a-4ba1-86ac-1462afd7a416",
   "metadata": {},
   "source": [
    "### Creating input sequences and corresponding target characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce2ee5e-99f6-4f6a-af9d-5fb6f2a40571",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "target_characters = []\n",
    "\n",
    "for word in words:\n",
    "    for i in range(len(word) - 1):\n",
    "        input_sequences.append(word[:i+1])\n",
    "        target_characters.append(word[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58812b8-c5b0-4fa2-bf69-3d569a7053f0",
   "metadata": {},
   "source": [
    "### Creating a mapping of characters to numerical indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4130bd2-2b94-4706-b930-23f18f720225",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(set(target_characters))}\n",
    "num_classes = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111335d-6c43-48b8-9358-b66389ed30a2",
   "metadata": {},
   "source": [
    "### Converting input sequences to numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a715724c-d2cf-434e-8a69-c7ec2eae3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max(len(seq) for seq in input_sequences)\n",
    "X = np.zeros((len(input_sequences), max_seq_length), dtype=np.int32)\n",
    "for i, seq in enumerate(input_sequences):\n",
    "    for j, char in enumerate(seq):\n",
    "        X[i, j] = char_to_idx[char]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05506f3-c2a4-49d9-ae77-35f21add447f",
   "metadata": {},
   "source": [
    "### Convert target characters to numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab3a764a-b6e6-415a-b780-b6cca368ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([char_to_idx[char] for char in target_characters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff4ae583-1ea5-4343-b0db-b26b4185681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd40c3-433f-4f63-b26f-f5b7e94e4259",
   "metadata": {},
   "source": [
    "### After data split creating an ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e756db6-900d-4090-948c-65ce0a626e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(num_classes, 64, input_length=max_seq_length),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b067e1-fcc2-40b7-8dd4-b26ee356cd87",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c627334-e330-4fa6-90db-d596adbaea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5df467df-9b0f-4300-af66-84f224aaf431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 28, 64)            1664      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                1690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,378\n",
      "Trainable params: 36,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e75ff-aff9-45d8-a9d5-2d635a4434c8",
   "metadata": {},
   "source": [
    "### Model Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0511772e-35f3-4ec7-b9d3-84394386f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37949/37949 [==============================] - 744s 20ms/step - loss: 2.4574 - accuracy: 0.2319 - val_loss: 2.2970 - val_accuracy: 0.2793\n",
      "Epoch 2/10\n",
      "37949/37949 [==============================] - 737s 19ms/step - loss: 2.2450 - accuracy: 0.2947 - val_loss: 2.2156 - val_accuracy: 0.3043\n",
      "Epoch 3/10\n",
      "37949/37949 [==============================] - 692s 18ms/step - loss: 2.1890 - accuracy: 0.3110 - val_loss: 2.1861 - val_accuracy: 0.3135\n",
      "Epoch 4/10\n",
      "37949/37949 [==============================] - 672s 18ms/step - loss: 2.1610 - accuracy: 0.3189 - val_loss: 2.1557 - val_accuracy: 0.3215\n",
      "Epoch 5/10\n",
      "37949/37949 [==============================] - 690s 18ms/step - loss: 2.1438 - accuracy: 0.3245 - val_loss: 2.1480 - val_accuracy: 0.3240\n",
      "Epoch 6/10\n",
      "37949/37949 [==============================] - 703s 19ms/step - loss: 2.1316 - accuracy: 0.3283 - val_loss: 2.1371 - val_accuracy: 0.3281\n",
      "Epoch 7/10\n",
      "37949/37949 [==============================] - 712s 19ms/step - loss: 2.1229 - accuracy: 0.3306 - val_loss: 2.1280 - val_accuracy: 0.3297\n",
      "Epoch 8/10\n",
      "37949/37949 [==============================] - 676s 18ms/step - loss: 2.1160 - accuracy: 0.3325 - val_loss: 2.1354 - val_accuracy: 0.3278\n",
      "Epoch 9/10\n",
      "37949/37949 [==============================] - 720s 19ms/step - loss: 2.1100 - accuracy: 0.3348 - val_loss: 2.1220 - val_accuracy: 0.3319\n",
      "Epoch 10/10\n",
      "37949/37949 [==============================] - 8231s 217ms/step - loss: 2.1051 - accuracy: 0.3365 - val_loss: 2.1175 - val_accuracy: 0.3331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2387b8edcc0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edb9cb99-32b2-42b4-b0f9-7f05411adade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11860/11860 [==============================] - 109s 9ms/step - loss: 2.1138 - accuracy: 0.3326\n",
      "Test accuracy: 0.33261746168136597\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f6a61-ba73-4486-9ed5-948421402187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee373ffb-9401-432f-b98d-6d8bcb673f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd569ccc-dca3-4d37-a31d-dc87a5e5d64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec65a9f-98d7-49a3-8827-d01743d294f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83720090-3355-4e61-8711-83d20e242606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fd66d-a5ad-458a-885e-a060e2c92df2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
